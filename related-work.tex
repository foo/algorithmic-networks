\todo{Megre}

\subsection{Related Work on Static Mapping}


There has recently been much interest in programming models and distributed
system architectures for the processing and analysis of big data (e.g.~\cite{nodb,mapreduce,shark}). The model studied in
this paper is motivated by MapReduce~\cite{mapreduce} like batch-processing applications, also known
from the popular open-source implementation \emph{Apache Hadoop}.
These applications
generate large amounts of network traffic~\cite{orchestra,talk-about,amazonbw},
and over the last years, several systems have been proposed which provide
a provable network performance, also in shared cloud environments, by supporting
relative~\cite{faircloud,elasticswitch,seawall}
or, as in the case of our paper, \emph{absolute}~\cite{oktopus,secondnet,drl,gatekeeper,proteus} bandwidth reservations
between the virtual machines.

The most popular virtual network abstraction for batch-processing applications today is the \emph{virtual cluster},
introduced in the Oktopus paper~\cite{oktopus}, and later studied by many others~\cite{talk-about,infocom16,ccr15emb,proteus}. In particular, Proteus \cite{proteus} improves
upon the Oktopus~\cite{oktopus} embedding algorithm of fat-trees and makes the case
for a time-adaptive embedding. The Kraken system~\cite{infocom16} is based on an optimal
embedding algorithm of fat-trees and and allows to elastically scale both link as well as
node resources. In~\cite{ccr15emb}, Rost et al.~show that the virtual cluster abstraction
can even be embedded on general graphs in polynomial time, and initiate the algorithmic study
of a Hose interpretation of the virtual cluster abstraction.

Several heuristics have been developed to compute ``good'' embeddings of virtual clusters: embeddings
with small footprints (minimal bandwidth reservation costs).
The virtual network embedding problem has also been studied for more general graph abstractions
(e.g., motivated by wide-area networks).~\cite{boutaba-survey,fischer-survey}


From a theoretical perspective, the virtual network embedding problem can be seen as a generalization
of classic VPN graph embedding problems~\cite{Goyal2008,gupta2001provisioning},
in the sense that in virtual network embedding problems, also the embedding endpoints are flexible. In this respect, the virtual network embedding problem can also be seen as a generalization of the
classic NP-hard Minimum Linear Arrangement problem which asks for the
embedding of guest graphs on a simple \emph{line topology} (rather than tree-like topologies as
studied in this paper)~\cite{mla,mla-survey}.

However, to the best of our knowledge, we are the first to provide an algorithmic
study of the virtual cluster embedding problem which takes into account
data locality as well as the possibility to select replicas---aspects which so far have only
been studied from a best-effort perspective and using coarse-grained metrics (e.g., same rack or same server), thus limiting the flexibility of the
system~\cite{local-schedule-1,local-schedule-2,local-schedule-3}.

\noindent \textbf{Bibliographic Note.} A preliminary version of this paper appeared
at the 23rd IEEE International Conference on Network Protocols (ICNP), 2015~\cite{icnp15loc}.



\subsection{Related Work on Dynamic Mapping}


The static offline version of our problem, i.e., a problem variant where
migration is not allowed, where all requests are known in advance, and where
the goal is to find best node assignment to $\ell$ clusters, is known as the
$\ell$-balanced graph partitioning problem. The problem is 
NP-complete, and cannot even be approximated within any finite factor unless P
= NP~\cite{AndRae06}. The static variant where $n/\ell = 2$ corresponds to a
maximum matching problem, which is polynomial-time solvable. The static
variant where $\ell = 2$ corresponds to the minimum bisection problem, which
is already NP-hard~\cite{GaJoSt76}. Its approximation was studied in a long
line of work~\cite{SarVaz95,ArKaKa99,FeKrNi00,FeiKra02,KraFei06,Raec08} and
the currently best approximation ratio of $O(\log n)$ was given by
R{\"{a}}cke~\cite{Raec08}. The $O(\log^{3/2} n)$-approximation given by
Krauthgamer and Feige~\cite{KraFei06} can be extended to~general $\ell$, but
the running time becomes exponential in~$\ell$.

The inaproximability of the static variant for general values of $\ell$
motivated research on the bicriteria variant, which can be seen as the offline
counterpart of our cluster-size augmentation approach. Here, the~goal
is~to~develop $(\ell,\delta)$-balanced graph partitioning, where the graph has
to be partitioned into $\ell$ components of~size less than $\delta \cdot (n /
\ell)$ and the cost of the cut is compared to the optimal (non-augmented)
solution where all components are of size $n / \ell$. The variant where
$\delta \geq 2$ was considered in
\cite{LeMaTr90,SimTen97,EvNaRS00,EvNaRS99,KrNaSc09}. So far the best result is
an $O(\!\sqrt{\log n \cdot \log \ell})$-approximation by Krauthgamer et
al.~\cite{KrNaSc09}, which builds on ideas from the $O(\!\sqrt{\log
n})$-approximation algorithm for balanced cuts by Arora et al.~\cite{ArRaVa09}.
For smaller values of $\delta$, i.e., when $\delta = 1 + \eps$ with a fixed
$\eps > 0$, Andreev and R{\"{a}}cke gave an $O(\log^{1.5} n / \eps^2)$
approximation~\cite{AndRae06}, which was later improved to $O(\log n)$ by
Feldmann and Foschini ~\cite{FelFos15}.

The BRP problem considered in this paper was not previously studied. However,
it bears some resemblance to the classic online problems; below we highlight
some of them.

Our model is related to online
paging~\cite{SleTar85,FKLMSY91,McGSle91,AcChNo00}, sometimes also referred to
as online caching, where requests for data items (nodes) arrive over time and
need to be served from a cache of finite capacity, and where the number of
cache misses must be minimized. Classic problem variants usually boil down to
finding a smart eviction strategy, such as Least Recently Used (LRU). In our
setting, requests can be served remotely (i.e., without fetching the
corresponding nodes to a single cluster). In this light, our model is more
reminiscent of caching models \emph{with
bypassing}~\cite{EpImLN11,EpImLN15,Irani02}. Nonetheless, we show that BRP is
capable of emulating online paging.

The BRP problem is an example of a non-uniform problem~\cite{KaMaMO94}: the
cost of changing the state is higher than the cost of serving a single
request. This requires finding a~good trade-off between serving requests
remotely (at a low but repeated communication cost) or migrating nodes into a
single cluster (entailing a potentially high one-time cost). Many
online problems exhibit this so called \emph{rent-or-buy} property, e.g., ski
rental problem~\cite{KaMaMO94,LoPaRa08}, relaxed metrical task
systems~\cite{BaChIn01}, file migration~\cite{BaChIn01,BiByMu17}, distributed
data management~\cite{BaFiRa95,AwBaFi93,AwBaFi98}, or rent-or-buy network
design~\cite{AwAzBa04,Umboh15,FeWiLe16}.

There are two major differences between BRP and the problems listed above.
First, these problems typically maintain some configuration of servers or
bought infrastructure and upon a new request (whose cost typically depends on
the distance to the infrastructure), decide about its reconfiguration (e.g.,
server movement or purchasing additional links). In contrast, in our model,
\emph{both} end-points of a communication request are subject to optimization.
Second, in the BRP problem a request reveals only very limited information
about the optimal configuration to serve it: There exist relatively long
sequences of requests that can be served with zero cost from a fixed
configuration. Not only can the set of such configurations be very large, but
such configurations may also differ significantly from each other.

\subsection{Related Work on Caching / Cache Management / Resource Management}

Our formal model is a novel variant of competitive paging, a~classic online
problem. In the framework of the competitive analysis, the paging problem was
first analyzed  by Sleator and Tarjan~\cite{competitive-analysis}, who showed
that algorithms \textsc{Least-Recently-Used}, \textsc{First-In-First-Out} and
\textsc{Flush-When-Full} are $\kALG / (\kALG - \kOPT + 1)$-competitive 
and no deterministic algorithm can beat this ratio. In the non-augmented case
when $\kALG = \kOPT = k$, the competitive ratio is simply $k$.

The simple paging problem was later generalized to allow different fetching
costs (weighted paging)~\cite{double-coverage,young-paging-greedy-dual} and
additionally different item sizes (file caching)~\cite{young-paging-landlord},
with the same competitive ratio. Asymptotically same results can be achieved
when bypassing is allowed (see \cite{caching-rejection-penalties,paging-irani}
and references therein). With randomization, the competitive ratio can be
reduced to $O(\log k)$ even for file caching~\cite{generalized-caching-optimal}. 
The lower bound for randomized algorithms is $H_k = 
\Theta(\log k)$~\cite{paging-mark} and is matched by known paging
algorithms~\cite{paging-optimal-easy,paging-optimal-difficult}.

To the best of our knowledge, the variant of caching, where fetching items to
the cache is not allowed unless some other items are cached (e.g., because of 
tree dependencies) was 
not considered previously in the framework of competitive analysis. Note that
there is a seemingly related problem called restricted
caching~\cite{restricted-caching} (there are also its variants called matroid
caching~\cite{matroid-caching} or companion caching~\cite{companion-caching}).
Despite naming similarities, the restricted caching model is completely
different from ours: there the restriction is that each item can be placed only in
a~restricted set of cache locations.

